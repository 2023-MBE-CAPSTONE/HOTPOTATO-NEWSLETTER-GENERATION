{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) \n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 담론 구성\n",
    "    - zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CONTENT = \"\"\"\n",
    "각각의 키워드를 이용하여 이슈에 대한 담론을 만들어라.\n",
    "이슈 : 의대 정원 확대\n",
    "긍정입장 대표 키워드: 찬성, 특권, 이기적\n",
    "부정입장 대표 키워드: 반대, 파업, 피부과\n",
    "\n",
    "긍정 담론 :\n",
    "부정 담론 :\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정 담론 :\n",
      "야, 너도 알겠지만 지금 의대 정원 확대 이슈가 크게 떠오르고 있어. 그런데 이걸 왜 찬성해야 하는지 아직 모르겠다고? 그럼 내가 좀 알려줄게. \n",
      "\n",
      "첫 번째로, 의대 정원 확대는 의사들의 특권을 깨는 거야. 의사가 되려면 얼마나 많은 시간과 노력을 투자해야 하는지 알지? 그런데 그런 노력에 비해 의사들이 받는 대우는 너무 과하다고 생각하지 않아? 의대 정원을 확대하면, 의사들의 특권이 깨지면서 더 많은 사람들이 의사가 될 기회를 얻게 되는 거야.\n",
      "\n",
      "두 번째로, 의대 정원 확대는 의사들의 이기적인 행동을 막을 수 있어. 의사들이 파업을 하면서 환자들을 피해 보는 일이 없도록 하는 거지. 의대 정원을 확대하면, 의사들이 파업을 해도 환자들에게 큰 피해가 가지 않게 될 거야.\n",
      "\n",
      "부정 담론 :\n",
      "하지만, 의대 정원 확대에 반대하는 사람들도 많아. 그 이유를 알고 싶다고? 그럼 내가 좀 알려줄게.\n",
      "\n",
      "첫 번째로, 의대 정원 확대는 의사들의 파업을 부추기는 거야. 의사들이 파업을 하면서 환자들에게 피해를 주는 건 알겠지? 그런데 의대 정원을 확대하면, 의사들이 파업을 해도 큰 문제가 없다고 생각할 수 있어. 그럼 의사들이 더 자주 파업을 하게 되는 거지.\n",
      "\n",
      "두 번째로, 의대 정원 확대는 피부과 같은 특정 과에만 의사들이 몰리게 만드는 거야. 의대 정원을 확대하면, 의사가 되기 쉬워지니까 피부과 같은 수익이 높은 과에만 의사들이 몰리게 되는 거야. 그럼 다른 과의 의사들은 부족해지고, 그로 인해 환자들이 피해를 보게 되는 거지.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"너는 이슈와 이슈를 잘 나타내는 대표 키워드를 기반으로 이슈에 대한 반론을 친근한 반말 뉴스레터로 만드는 작가야.\"},\n",
    "        {\"role\": \"user\", \"content\": CONTENT},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"positive_article\": string  // 긍정 담론에 대한 뉴스레터\\n\\t\"negative_article\": string  // 부정 담론에 대한 뉴스레터\\n}\\n```'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"positive_article\", description=\"긍정 담론에 대한 뉴스레터\"),\n",
    "    ResponseSchema(name=\"negative_article\", description=\"부정 담론에 대한 뉴스레터\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "#시스템 역할 지정하기\n",
    "template = \"너는 이슈와 이슈를 잘 나타내는 대표 키워드를 기반으로 이슈에 대한 반론을 친근한 반말 뉴스레터로 만드는 작가야.\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "human_template = \"\"\"\n",
    "각각의 키워드를 이용하여 이슈에 대한 담론을 만들어라. \n",
    "이슈 : {issue_name} \n",
    "긍정입장 대표 키워드: {positive_keyword}\n",
    "부정입장 대표 키워드: {negative_keyword}\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "query = chat_prompt.format_messages(output_language=\"Korean\",\n",
    "                            issue_name=\"의대 정원 확대\",\n",
    "    positive_keyword=[\"찬성\", \"특권\", \"이기적\"],\n",
    "    negative_keyword=[\"반대\", \"파업\", \"피부과\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \n",
    "                 max_tokens=2048,             # 최대 토큰수\n",
    "                 model_name='gpt-4',  # 모델명\n",
    "                )\n",
    "ans = llm.predict_messages(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n\"positive_paragraph\": \"야, 너도 알겠지만, 우리나라 의대 정원 확대 이슈가 요즘 뜨거워. 많은 사람들이 \\'찬성\\'하는 이유는 뭐냐면, 의사들의 \\'특권\\'을 깨기 위해서야. 의사들이 너무 \\'이기적\\'으로 행동하고 있어서, 그게 사회적 불평등을 만들어내고 있거든. 의대 정원을 확대하면, 의사들의 특권을 깨고, 더 많은 사람들이 의사가 될 수 있어. 그럼 의료 서비스가 더 좋아지고, 의료비도 낮아질 수 있지 않겠냐? 그래서 나는 의대 정원 확대를 찬성하는 거야.\",\\n\\n\"negative_paragraph\": \"하지만, 다른 사람들은 이 의대 정원 확대에 \\'반대\\'하고 있어. 왜냐하면, 의대 정원 확대가 의사들의 \\'파업\\'을 초래하고 있거든. 의사들이 파업을 하면, 환자들이 피해를 보는 건 아는거지? 그리고, 의대 정원 확대가 \\'피부과\\' 같은 특정 전문과를 선호하는 문제도 있어. 의대 정원을 확대하면, 의사들이 피부과 같은 고수익 전문과를 선호하게 되고, 그럼 다른 전문과 의사들이 부족해지는 문제가 생기지 않겠냐? 그래서 나는 의대 정원 확대에 반대하는 거야.\"\\n}')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for HumanMessage\ncontent\n  str type expected (type=type_error.str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m LLMChain\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# 질의\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[답변]: \u001b[39m\u001b[39m{\u001b[39;00mllm\u001b[39m.\u001b[39;49mpredict(final_prompt)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# chatchain = LLMChain(llm=llm, prompt=final_prompt)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hwanghyejeong/Documents/projects/hot-potato-newsletter-generation/make-news-letter.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# chatchain.run(input_language=\"English\", output_language=\"Korean\")\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/hot-potato-newsletter-generation-G29dl3HG/lib/python3.11/site-packages/langchain/chat_models/base.py:636\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[0;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    635\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[0;32m--> 636\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m([HumanMessage(content\u001b[39m=\u001b[39;49mtext)], stop\u001b[39m=\u001b[39m_stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    637\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/hot-potato-newsletter-generation-G29dl3HG/lib/python3.11/site-packages/langchain/load/serializable.py:97\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/hot-potato-newsletter-generation-G29dl3HG/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[39m'\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for HumanMessage\ncontent\n  str type expected (type=type_error.str)"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# 객체 생성\n",
    "llm = ChatOpenAI(temperature=0,               # 창의성 (0.0 ~ 2.0) \n",
    "                 max_tokens=2048,             # 최대 토큰수\n",
    "                 model_name='gpt-4',  # 모델명\n",
    "                )\n",
    "\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# 질의\n",
    "print(f'[답변]: {llm.predict(final_prompt)}')\n",
    "# chatchain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "# chatchain.run(input_language=\"English\", output_language=\"Korean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hot-potato-mail-FVYmygE6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
